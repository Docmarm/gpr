import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime, date
import os
import re
import tempfile
from io import BytesIO
from pathlib import Path
from PyPDF2 import PdfReader
import logging # Ajout pour un meilleur logging
# Import the main function from carburant.py
from carburant import main as module_carburant_main

# Configuration du logging (optionnel mais utile pour le debug)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

################################
# Fonctions / classes utilitaires
################################

def to_excel(df: pd.DataFrame) -> bytes:
    """Convertit un DataFrame en fichier Excel binaire en m√©moire."""
    output = BytesIO()
    # Utilisation de try-except pour g√©rer les erreurs potentielles d'√©criture Excel
    try:
        with pd.ExcelWriter(output, engine='openpyxl') as writer:
            df.to_excel(writer, index=False, sheet_name='Sheet1')
        processed_data = output.getvalue()
        return processed_data
    except Exception as e:
        st.error(f"Erreur lors de la g√©n√©ration du fichier Excel : {e}")
        return b"" # Retourne des bytes vides en cas d'erreur

def is_non_working_hour(dt: datetime, start_non_work: int, end_non_work: int) -> bool:
    """
    D√©termine si l'heure de l'objet datetime est en dehors des heures ouvr√©es.
    Les heures ouvr√©es sont de end_non_work (inclus) √† start_non_work (exclus).
    """
    h = dt.hour
    # Cas standard: ex: start=18, end=8. Heures non ouvr√©es: >= 18 OU < 8
    if start_non_work > end_non_work:
        return h >= start_non_work or h < end_non_work
    # Cas o√π les heures non ouvr√©es sont dans la m√™me journ√©e: ex: start=8, end=17. Non ouvr√©es: >= 8 ET < 17
    elif start_non_work < end_non_work:
         return start_non_work <= h < end_non_work
    # Cas o√π start == end, on suppose que tout est ouvr√© (ou non ouvr√© si on veut, ici on dit ouvr√©)
    else:
        return False

def is_weekend(dt: datetime) -> bool:
    """V√©rifie si la date est un Samedi (5) ou Dimanche (6)."""
    return dt.weekday() >= 5

class AutorouteAnalyzer:
    def __init__(self):
        self.all_transactions = []
        self.card_owners = {}
        self.raw_df = pd.DataFrame()
        self.analysis_results = {}
        self.start_non_work_hour = 18 # Default value
        self.end_non_work_hour = 8    # Default value
        self.is_data_loaded = False

    def set_non_working_hours(self, start: int, end: int):
        """D√©finit les heures non ouvr√©es."""
        self.start_non_work_hour = start
        self.end_non_work_hour = end
        # Recalculer si les donn√©es sont d√©j√† charg√©es
        if not self.raw_df.empty:
            self._enrich_dataframe()

    def load_card_owners(self, excel_file_path: str):
        """Charge la liste des propri√©taires de cartes depuis un fichier Excel."""
        try:
            # S'assurer que le type est bien str pour √©viter les probl√®mes avec les num√©ros longs
            df = pd.read_excel(excel_file_path, dtype={'N¬∞CARTERAPIDO': str})
            # Nettoyage potentiel des espaces ou caract√®res non visibles
            df['N¬∞CARTERAPIDO'] = df['N¬∞CARTERAPIDO'].astype(str).str.strip()
            df.dropna(subset=['N¬∞CARTERAPIDO', 'USER'], inplace=True) # Ignorer lignes avec N¬∞ ou USER manquant
            self.card_owners = df.set_index('N¬∞CARTERAPIDO')['USER'].to_dict()
            logging.info(f"Charg√© {len(self.card_owners)} propri√©taires de cartes depuis {excel_file_path}")
        except FileNotFoundError:
            st.error(f"Erreur: Le fichier Excel '{excel_file_path}' n'a pas √©t√© trouv√©.")
            self.card_owners = {}
        except Exception as e:
            st.error(f"Erreur lors du chargement ou de la lecture du fichier Excel des cartes: {e}")
            logging.error(f"Erreur chargement liste des cartes: {e}", exc_info=True)
            self.card_owners = {}

    def parse_pdf_content(self, pdf_content: BytesIO, account_number: str):
        """Parse le contenu d'un fichier PDF et extrait les transactions."""
        try:
            reader = PdfReader(pdf_content)
            text = ""
            for page in reader.pages:
                extracted = page.extract_text()
                if extracted:
                    text += extracted + "\n"

            # Regex plus robuste pour capturer la ligne de transaction
            # Format: JJ/MM/AAAA HH:MM:SS TYPE ... MONTANT XOF ...
            # Le montant peut avoir des points comme s√©parateurs de milliers et une virgule d√©cimale
            transaction_pattern = re.compile(
                r"(\d{2}/\d{2}/\d{4})\s+"       # Date
                r"(\d{2}:\d{2}:\d{2})\s+"       # Heure
                r"(\S+)\s+"                     # Type (non-space characters)
                r"(.*?)\s+"                     # Reste de la ligne (non-greedy)
                r"([\d.,]+)\s+XOF"              # Montant suivi de XOF
            )

            account_number = str(account_number).strip()
            direction = self.card_owners.get(account_number, "Direction Non Identifi√©e")
            parsed_count = 0

            for line in text.split('\n'):
                match = transaction_pattern.search(line)
                if match:
                    date_str, time_str, movement_type, _, amount_str = match.groups()
                    date_time_str = f"{date_str} {time_str}"
                    amount = 0.0
                    try:
                        # Nettoyer le montant: supprimer les points, remplacer la virgule par un point
                        amount = float(amount_str.replace('.', '').replace(',', '.'))
                    except ValueError:
                        logging.warning(f"Impossible de parser le montant '{amount_str}' pour le compte {account_number} sur la ligne: {line}")
                        amount = 0.0 # Ou choisir de lever une erreur ou skipper

                    try:
                        dt = datetime.strptime(date_time_str, '%d/%m/%Y %H:%M:%S')
                        transaction = {
                            'account': account_number,
                            'datetime': dt,
                            'type': movement_type.capitalize(), # Standardiser la casse
                            'amount': amount,
                            'direction': direction if direction else "Direction Non Identifi√©e"
                        }
                        self.all_transactions.append(transaction)
                        parsed_count += 1
                    except ValueError:
                        logging.warning(f"Format de date/heure invalide '{date_time_str}' pour le compte {account_number} sur la ligne: {line}")
                        continue # Ignore cette ligne si la date n'est pas valide

            logging.info(f"PDF {account_number}: {parsed_count} transactions pars√©es.")

        except Exception as e:
            st.error(f"Erreur lors du parsing du PDF pour le compte {account_number}: {e}")
            logging.error(f"Erreur parsing PDF {account_number}: {e}", exc_info=True)

    def _enrich_dataframe(self):
        """Ajoute des colonnes calcul√©es au DataFrame brut."""
        if self.raw_df.empty:
            return

        df = self.raw_df # Travaille sur la r√©f√©rence interne

        if 'datetime' not in df.columns:
             logging.error("La colonne 'datetime' est manquante dans le DataFrame brut.")
             st.error("Erreur interne: Colonne 'datetime' non trouv√©e.")
             return

        df['date'] = df['datetime'].dt.date
        df['hour'] = df['datetime'].dt.hour
        df['month'] = df['datetime'].dt.strftime('%Y-%m')
        df['year'] = df['datetime'].dt.year
        df['day_of_week_num'] = df['datetime'].dt.weekday # Pour tri
        df['is_weekend'] = df['datetime'].apply(is_weekend)

        jours_fr = { 0: 'Lundi', 1: 'Mardi', 2: 'Mercredi', 3: 'Jeudi', 4: 'Vendredi', 5: 'Samedi', 6: 'Dimanche'}
        df['day_of_week'] = df['day_of_week_num'].map(jours_fr)

        # Appliquer le calcul bas√© sur les heures d√©finies dans l'instance
        df['is_non_working_hours'] = df['datetime'].apply(
            lambda dt: is_non_working_hour(dt, self.start_non_work_hour, self.end_non_work_hour)
        )

        # Assigner le DataFrame enrichi √† l'attribut de classe
        self.raw_df = df


    def finalize_data_loading(self):
        """Cr√©e le DataFrame final √† partir des transactions pars√©es et l'enrichit."""
        if not self.all_transactions:
            st.warning("Aucune transaction n'a √©t√© trouv√©e ou pars√©e.")
            self.raw_df = pd.DataFrame()
            self.is_data_loaded = False
            return

        self.raw_df = pd.DataFrame(self.all_transactions)
        # Conversion explicite en datetime si ce n'est pas d√©j√† fait
        self.raw_df['datetime'] = pd.to_datetime(self.raw_df['datetime'])
        # Tri par date pour la coh√©rence
        self.raw_df.sort_values(by='datetime', inplace=True)
        self._enrich_dataframe() # Enrichir avec les colonnes calcul√©es
        self.is_data_loaded = True
        logging.info(f"DataFrame final cr√©√© avec {len(self.raw_df)} transactions.")


    def run_all_analyses(self, inactivity_threshold=90):
        """Ex√©cute toutes les analyses et stocke les r√©sultats."""
        if self.raw_df.empty:
            st.warning("Impossible d'ex√©cuter les analyses : aucune donn√©e charg√©e.")
            self.analysis_results = {}
            return {}

        df = self.raw_df # Utilise le DF enrichi de la classe

        # Utilisation de m√©thodes pr√©fix√©es par _ pour indiquer qu'elles sont "priv√©es" √† la classe
        self.analysis_results['passages_mensuels_par_direction'] = self._analyze_passages_mensuels_par_direction(df)
        self.analysis_results['nb_passages_par_carte_direction'] = self._analyze_nb_passages_par_carte_direction(df)
        self.analysis_results['nb_cartes_par_direction'] = self._analyze_nb_cartes_par_direction(df)
        self.analysis_results['cartes_weekend_heures_non_ouvrees'] = self._analyze_cartes_weekend_heures_non_ouvrees(df)

        # Analyse d'activit√©
        activite_summary, activite_details = self._analyze_card_activity(df, inactivity_threshold)
        self.analysis_results['activite_cartes_resume'] = activite_summary
        self.analysis_results['activite_cartes_details'] = activite_details

        # Analyses suppl√©mentaires
        self.analysis_results['recharges_mensuelles'] = self._analyze_recharges_mensuelles(df)
        self.analysis_results['recharges_par_direction'] = self._analyze_recharges_par_direction(df)
        self.analysis_results['heures_pointe_par_direction'] = self._analyze_peak_hours_per_direction(df)

        logging.info("Toutes les analyses ont √©t√© ex√©cut√©es.")
        return self.analysis_results

    def get_filtered_data(self, start_date=None, end_date=None, directions=None, types=None, weekend_filter=None, non_working_hours_filter=None, search_term=None):
        """Filtre le DataFrame principal selon les crit√®res fournis."""
        if self.raw_df.empty:
            return pd.DataFrame()

        filtered_df = self.raw_df.copy()

        if start_date:
            filtered_df = filtered_df[filtered_df['datetime'].dt.date >= start_date]
        if end_date:
            filtered_df = filtered_df[filtered_df['datetime'].dt.date <= end_date]
        if directions and "Tous" not in directions:
            filtered_df = filtered_df[filtered_df['direction'].isin(directions)]
        if types:
            filtered_df = filtered_df[filtered_df['type'].isin(types)]

        # Filtre Weekend / Heures non ouvr√©es (peut √™tre 'Oui', 'Non', ou None/Tous)
        if weekend_filter == 'Oui':
             filtered_df = filtered_df[filtered_df['is_weekend']]
        elif weekend_filter == 'Non':
             filtered_df = filtered_df[~filtered_df['is_weekend']]

        if non_working_hours_filter == 'Oui':
             filtered_df = filtered_df[filtered_df['is_non_working_hours']]
        elif non_working_hours_filter == 'Non':
             filtered_df = filtered_df[~filtered_df['is_non_working_hours']]

        # Combinaison des filtres Weekend ET/OU Heures non ouvr√©es si n√©cessaire
        # Exemple: Si l'utilisateur s√©lectionne "Weekend" ET "Heures non ouvr√©es" dans un multiselect
        # on pourrait vouloir les transactions qui sont SOIT l'un SOIT l'autre.
        # La logique actuelle filtre s√©quentiellement. Adapter si besoin.

        if search_term:
            filtered_df = filtered_df[
                filtered_df['account'].astype(str).str.contains(search_term, case=False, na=False) |
                filtered_df['amount'].astype(str).str.contains(search_term, case=False, na=False) # Recherche aussi sur le montant
            ]

        return filtered_df

    # --- M√©thodes d'Analyse Priv√©es ---

    def _analyze_card_activity(self, df: pd.DataFrame, inactivity_threshold=90):
        """Analyse l'activit√© des cartes."""
        if df.empty:
            # Retourne des DataFrames vides avec les colonnes attendues si aucune donn√©e
            summary_cols = ['Direction', 'Total_cartes', 'Cartes_actives', 'Cartes_inactives']
            details_cols = ['Num√©ro_carte_Rapido', 'Direction', 'Statut', 'Derni√®re_transaction', 'Jours_depuis_derniere_transaction']
            return pd.DataFrame(columns=summary_cols), pd.DataFrame(columns=details_cols)

        # Utiliser la date maximale des transactions comme r√©f√©rence si disponible, sinon aujourd'hui
        reference_date = df['datetime'].max() if not df.empty else pd.Timestamp.now()
        reference_date = pd.to_datetime(reference_date) # Ensure it's a Timestamp

        all_known_cards = set(self.card_owners.keys())
        active_cards_in_df = set(df['account'].unique())

        # Cartes connues mais sans transaction dans le df actuel
        inactive_cards = all_known_cards - active_cards_in_df

        # Pr√©parer les d√©tails pour TOUTES les cartes connues
        details_data = []
        for card in all_known_cards:
            card_transactions = df[df['account'] == card]
            direction = self.card_owners.get(card, "Inconnu")
            last_transaction_time = None
            days_since_last = None
            status = "Inactive (jamais vue)" # Statut initial

            if not card_transactions.empty:
                last_transaction_time = card_transactions['datetime'].max()
                days_since_last = (reference_date - last_transaction_time).days
                if days_since_last <= inactivity_threshold:
                     status = "Active"
                else:
                     status = f"Inactive ({days_since_last} j)" # Plus pr√©cis
            elif card in inactive_cards:
                 # La carte est connue mais n'a *aucune* transaction dans le DF actuel.
                 # On ne peut pas calculer "days_since_last" √† partir de ce DF.
                 # On pourrait chercher la derni√®re date dans *toutes* les donn√©es si n√©cessaire,
                 # mais ici on se base sur le DF filtr√©/pass√© en argument.
                 status = "Inactive (p√©riode)" # Ou "Inactive (jamais vue)" si all_transactions est vide

            details_data.append({
                'Num√©ro_carte_Rapido': card,
                'Direction': direction,
                'Statut': status,
                'Derni√®re_transaction': last_transaction_time.date() if pd.notna(last_transaction_time) else None,
                'Jours_depuis_derniere_transaction': days_since_last if pd.notna(days_since_last) else None
            })

        details_df = pd.DataFrame(details_data)

        # Calculer le r√©sum√© bas√© sur les statuts d√©termin√©s
        summary_data = []
        # S'assurer que 'Direction' existe dans details_df
        if 'Direction' in details_df.columns:
            for direction in sorted(details_df['Direction'].unique()):
                dir_subset = details_df[details_df['Direction'] == direction]
                total_dir = len(dir_subset)
                active_dir = len(dir_subset[dir_subset['Statut'] == "Active"])
                inactive_dir = total_dir - active_dir
                summary_data.append({
                    'Direction': direction,
                    'Total_cartes': total_dir,
                    'Cartes_actives': active_dir,
                    'Cartes_inactives': inactive_dir
                })
        else:
            # G√©rer le cas o√π details_df pourrait √™tre vide ou sans colonne Direction
             logging.warning("Impossible de g√©n√©rer le r√©sum√© d'activit√© par direction.")


        # Ajouter une ligne 'Total'
        if not details_df.empty:
             total_cards = len(details_df)
             total_active = len(details_df[details_df['Statut'] == "Active"])
             total_inactive = total_cards - total_active
             summary_data.append({
                 'Direction': 'Total',
                 'Total_cartes': total_cards,
                 'Cartes_actives': total_active,
                 'Cartes_inactives': total_inactive
             })

        summary_df = pd.DataFrame(summary_data)

        return summary_df, details_df

    def _analyze_passages_mensuels_par_direction(self, df: pd.DataFrame):
        if df.empty or 'direction' not in df.columns or 'month' not in df.columns:
             return pd.DataFrame(columns=['direction', 'month', 'passages', 'nombre_passages_total']) # Structure vide
        group = df.groupby(['direction', 'month']).size().reset_index(name='passages')
        # Utiliser pivot_table pour g√©rer les mois manquants pour certaines directions
        pivot = pd.pivot_table(group, index='direction', columns='month', values='passages', fill_value=0)
        pivot['nombre_passages_total'] = pivot.sum(axis=1)
        pivot.reset_index(inplace=True)
        return pivot

    def _analyze_nb_passages_par_carte_direction(self, df: pd.DataFrame):
        if df.empty:
            return pd.DataFrame(columns=['Num√©ro carte Rapido', 'Direction', 'Nombre_passages_Transit', 'Montant_Transit', 'Montant_Promotion', 'Montant_Recharge'])

        # Filtrer par type AVANT de grouper
        df_transit = df[df['type'].str.lower() == 'transit']
        df_promo = df[df['type'].str.lower() == 'promotion']
        df_recharge = df[df['type'].str.lower() == 'recharge']

        # Aggregations
        transit_agg = df_transit.groupby(['account', 'direction']).agg(
            Nombre_passages_Transit=('datetime', 'size'), # Utilise une colonne non-NA
            Montant_Transit=('amount', 'sum')
        ).reset_index()

        promo_sum = df_promo.groupby(['account', 'direction'])['amount'].sum().reset_index(name='Montant_Promotion')
        recharge_sum = df_recharge.groupby(['account', 'direction'])['amount'].sum().reset_index(name='Montant_Recharge')

        # Merge en partant de toutes les combinaisons compte/direction vues dans transit (ou dans toutes les transactions si besoin)
        # Ici on part de transit_agg qui contient d√©j√† les comptes/directions ayant eu des transits
        merged = transit_agg
        if not promo_sum.empty:
            merged = pd.merge(merged, promo_sum, on=['account', 'direction'], how='left')
        else:
            merged['Montant_Promotion'] = 0

        if not recharge_sum.empty:
            merged = pd.merge(merged, recharge_sum, on=['account', 'direction'], how='left')
        else:
            merged['Montant_Recharge'] = 0

        merged.fillna(0, inplace=True)

        merged.rename(columns={'account': 'Num√©ro carte Rapido', 'direction': 'Direction'}, inplace=True)
        return merged[['Num√©ro carte Rapido', 'Direction', 'Nombre_passages_Transit', 'Montant_Transit', 'Montant_Promotion', 'Montant_Recharge']]

    def _analyze_nb_cartes_par_direction(self, df: pd.DataFrame):
         if df.empty or 'account' not in df.columns or 'direction' not in df.columns:
             return pd.DataFrame(columns=['direction', 'nb_cartes'])
         # Compte les cartes uniques par direction bas√©es sur les transactions pr√©sentes dans le df
         unique_combo = df[['account', 'direction']].drop_duplicates()
         grouped = unique_combo.groupby('direction').size().reset_index(name='nb_cartes_actives_periode') # Nom plus pr√©cis
         grouped.rename(columns={'direction': 'Direction'}, inplace=True)
         return grouped

    def _analyze_cartes_weekend_heures_non_ouvrees(self, df: pd.DataFrame):
        if df.empty:
            return pd.DataFrame(columns=['Num√©ro carte Rapido', 'Direction', 'Montant_Transit_Weekend', 'nb_weekend_transit', 'Montant_Transit_HH', 'nb_hors_horaires_transit'])

        # Filtrer uniquement les transactions de type 'Transit'
        df_transit = df[df['type'].str.lower() == 'transit'].copy()
        if df_transit.empty:
             return pd.DataFrame(columns=['Num√©ro carte Rapido', 'Direction', 'Montant_Transit_Weekend', 'nb_weekend_transit', 'Montant_Transit_HH', 'nb_hors_horaires_transit'])


        # Transactions Transit pendant le weekend
        weekend = df_transit[df_transit['is_weekend']]
        weekend_group = weekend.groupby(['account', 'direction']).agg(
            Montant_Transit_Weekend=('amount', 'sum'),
            nb_weekend_transit=('datetime', 'count') # ou 'size'
        ).reset_index()

        # Transactions Transit en heures non ouvr√©es ET PAS le weekend
        hh = df_transit[df_transit['is_non_working_hours'] & (~df_transit['is_weekend'])]
        hh_group = hh.groupby(['account', 'direction']).agg(
            Montant_Transit_HH=('amount', 'sum'),
            nb_hors_horaires_transit=('datetime', 'count')
        ).reset_index()

        # Fusionner les deux, en partant de toutes les cartes/directions ayant eu au moins une transaction WE ou HH
        # Utiliser une fusion externe pour garder toutes les cartes
        merged_wehh = pd.merge(weekend_group, hh_group, on=['account', 'direction'], how='outer')
        merged_wehh.fillna(0, inplace=True)
        merged_wehh.rename(columns={'account': 'Num√©ro carte Rapido', 'direction': 'Direction'}, inplace=True)

        # S'assurer que toutes les colonnes existent m√™me si l'un des groupes est vide
        for col in ['Montant_Transit_Weekend', 'nb_weekend_transit', 'Montant_Transit_HH', 'nb_hors_horaires_transit']:
             if col not in merged_wehh.columns:
                 merged_wehh[col] = 0

        return merged_wehh[['Num√©ro carte Rapido', 'Direction', 'Montant_Transit_Weekend', 'nb_weekend_transit', 'Montant_Transit_HH', 'nb_hors_horaires_transit']]

    def _analyze_recharges_mensuelles(self, df: pd.DataFrame):
        """Calcule le montant total des recharges par mois."""
        if df.empty or 'type' not in df.columns or 'month' not in df.columns or 'amount' not in df.columns:
            return pd.Series(dtype=float).rename('Montant_Recharge')
        recharges = df[df['type'].str.lower() == 'recharge']
        monthly_recharges = recharges.groupby('month')['amount'].sum().rename('Montant_Recharge')
        return monthly_recharges

    def _analyze_recharges_par_direction(self, df: pd.DataFrame):
        """Calcule le montant total des recharges par direction."""
        if df.empty or 'type' not in df.columns or 'direction' not in df.columns or 'amount' not in df.columns:
            return pd.DataFrame(columns=['Direction', 'Montant total des recharges'])
        recharges = df[df['type'].str.lower() == 'recharge']
        recharge_by_direction = recharges.groupby('direction')['amount'].sum().reset_index()
        recharge_by_direction.rename(columns={'direction': 'Direction', 'amount': 'Montant total des recharges'}, inplace=True)
        return recharge_by_direction

    def _analyze_peak_hours_per_direction(self, df: pd.DataFrame):
        """Analyse les heures de pointe (nombre de transactions) par direction."""
        if df.empty or 'direction' not in df.columns or 'hour' not in df.columns:
             return pd.DataFrame() # Retourne un DF vide
        # Compte les transactions par direction et par heure
        peak_hours = df.groupby(['direction', 'hour']).size().reset_index(name='transactions')
        # Pivoter pour avoir les heures en colonnes
        peak_hours_pivot = pd.pivot_table(peak_hours, index='direction', columns='hour', values='transactions', fill_value=0)
        # S'assurer que toutes les heures de 0 √† 23 sont pr√©sentes
        all_hours = list(range(24))
        peak_hours_pivot = peak_hours_pivot.reindex(columns=all_hours, fill_value=0)
        return peak_hours_pivot

    # --- Export ---
    def export_analyses_to_excel(self, filename: str, start_date: date = None, end_date: date = None, inactivity_threshold=90):
        """
        Exporte les donn√©es brutes filtr√©es et toutes les analyses (recalcul√©es sur les donn√©es filtr√©es)
        vers un fichier Excel multi-feuilles.
        """
        if self.raw_df.empty:
            st.error("Aucune donn√©e √† exporter.")
            return

        # 1. Filtrer les donn√©es brutes selon la plage de dates
        export_df = self.get_filtered_data(start_date=start_date, end_date=end_date)

        if export_df.empty:
             st.warning(f"Aucune transaction trouv√©e pour la p√©riode du {start_date} au {end_date}. L'export sera vide.")
             # Cr√©er un fichier Excel vide ou avec juste un message? Ici on arr√™te.
             return

        # 2. Recalculer TOUTES les analyses sur ce DataFrame filtr√©
        #    C'est co√ªteux mais garantit que l'export refl√®te exactement la p√©riode s√©lectionn√©e.
        #    Alternative: Exporter les analyses pr√©-calcul√©es et indiquer la p√©riode globale de ces analyses.
        #    Ici, on choisit de recalculer pour la pr√©cision de l'export par p√©riode.
        temp_analyzer = AutorouteAnalyzer() # Cr√©er une instance temporaire pour ne pas affecter l'√©tat principal
        temp_analyzer.raw_df = export_df.copy() # Utiliser les donn√©es filtr√©es
        temp_analyzer.card_owners = self.card_owners # R√©utiliser la liste des cartes
        # Pas besoin de re-parser, juste recalculer les analyses
        export_analyses = temp_analyzer.run_all_analyses(inactivity_threshold=inactivity_threshold)


        # 3. Pr√©parer les DataFrames pour l'export (Renommage des colonnes, etc.)
        renamed_raw = export_df.rename(columns={
            'account': 'Num√©ro carte Rapido', 'datetime': 'Date et heure', 'type': 'Type transaction',
            'amount': 'Montant', 'direction': 'Direction', 'date': 'Date', 'hour': 'Heure',
            'month': 'Mois', 'is_weekend': 'Weekend', 'is_non_working_hours': 'Hors heures ouvr√©es',
            'day_of_week': 'Jour de la semaine', 'year': 'Ann√©e', 'day_of_week_num': 'Num Jour Semaine'
        })
        # S√©lectionner/r√©organiser les colonnes pour l'export brut si n√©cessaire
        cols_to_export = [
            'Num√©ro carte Rapido', 'Direction', 'Date et heure', 'Date', 'Heure', 'Jour de la semaine',
            'Type transaction', 'Montant', 'Mois', 'Ann√©e', 'Weekend', 'Hors heures ouvr√©es'
        ]
        # Garder seulement les colonnes qui existent r√©ellement dans le DF renomm√©
        renamed_raw_export = renamed_raw[[col for col in cols_to_export if col in renamed_raw.columns]]


        # 4. √âcrire dans le fichier Excel
        try:
            with pd.ExcelWriter(filename, engine='openpyxl') as writer:
                renamed_raw_export.to_excel(writer, sheet_name='Transactions_Filtrees', index=False)
                for sheet_name, data in export_analyses.items():
                    # G√©rer les Series (comme recharges_mensuelles) et DataFrames
                    if isinstance(data, pd.DataFrame):
                        if not data.empty:
                           # Remplacer les caract√®res non valides pour les noms de feuilles Excel si n√©cessaire
                           safe_sheet_name = re.sub(r'[\\/*?:\[\]]', '_', sheet_name)[:31] # Limite Excel
                           data.to_excel(writer, sheet_name=safe_sheet_name, index=False)
                    elif isinstance(data, pd.Series):
                         if not data.empty:
                             safe_sheet_name = re.sub(r'[\\/*?:\[\]]', '_', sheet_name)[:31]
                             data.to_frame().to_excel(writer, sheet_name=safe_sheet_name, index=True) # Garder l'index (ex: mois)
            logging.info(f"Export Excel '{filename}' g√©n√©r√© avec succ√®s pour la p√©riode {start_date} √† {end_date}.")
        except Exception as e:
             st.error(f"Erreur lors de l'√©criture du fichier Excel '{filename}': {e}")
             logging.error(f"Erreur √©criture Excel '{filename}': {e}", exc_info=True)


################################
# Mise en page et Styles (identique √† l'original)
################################
st.set_page_config(
    page_title="Analyse des passages Rapido",
    page_icon="üöó",
    layout="wide"
)

# Ajout d'un style CSS pour am√©liorer l'apparence
st.markdown("""
<style>
    /* Main app background */
    .main {
        background-color: #f9f9f9; /* Light grey background */
        padding: 1rem 2rem; /* More padding */
    }

    /* Card-like containers for charts and tables */
    .stDataFrame, .stPlotlyChart {
        background-color: white;
        border-radius: 8px; /* Softer corners */
        padding: 1rem 1.5rem; /* Consistent padding */
        box-shadow: 0 4px 8px rgba(0,0,0,0.05); /* Softer shadow */
        margin-bottom: 1.5rem; /* Space between elements */
        border: 1px solid #e0e0e0; /* Subtle border */
    }

    /* Sidebar styling */
    .sidebar .sidebar-content {
        background-color: #e8f0f4; /* Light blue-grey */
        padding: 1.5rem;
    }
    .sidebar .stFileUploader, .sidebar .stSlider, .sidebar .stMultiSelect {
        margin-bottom: 1rem;
    }
    .sidebar h3 {
        color: #004080; /* Dark blue */
        margin-top: 1.5rem;
    }

    /* Titles and Headers */
    h1, h2, h3 {
        color: #0056b3; /* Consistent blue for titles */
    }
    h1 {
        border-bottom: 2px solid #0056b3;
        padding-bottom: 0.5rem;
        margin-bottom: 1.5rem;
    }
    h2 {
        margin-top: 2rem;
        margin-bottom: 1rem;
    }
    h3 {
        margin-top: 1.5rem;
        margin-bottom: 0.8rem;
    }

    /* Buttons */
    .stButton>button {
        background-color: #007bff;
        color: white;
        border-radius: 5px;
        padding: 0.5rem 1rem;
        border: none;
        transition: background-color 0.2s ease;
    }
    .stButton>button:hover {
        background-color: #0056b3;
    }
    .stDownloadButton>button {
        background-color: #28a745; /* Green for download */
        color: white;
        border-radius: 5px;
        padding: 0.5rem 1rem;
        border: none;
        transition: background-color 0.2s ease;
    }
    .stDownloadButton>button:hover {
        background-color: #218838;
    }

    /* Metrics styling */
    .stMetric {
        background-color: #ffffff;
        border-left: 5px solid #007bff; /* Blue accent line */
        padding: 1rem;
        border-radius: 5px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.05);
        margin-bottom: 1rem; /* Add space below metrics */
    }
    .stMetric > div:nth-child(1) { /* Label */
        color: #555;
    }
    .stMetric > div:nth-child(2) { /* Value */
        font-size: 1.5em;
        font-weight: bold;
        color: #0056b3;
    }
     .stMetric > div:nth-child(3) { /* Delta (optional) */
        color: #28a745; /* Green delta, adjust if needed */
    }

    /* Specific info/warning boxes */
    .custom-info-box {
        padding: 10px;
        border-radius: 5px;
        background-color: #e0e0e0; /* Light grey */
        margin-bottom: 10px;
        border: 1px solid #c0c0c0;
    }
    .custom-status-active {
        color: #28a745; /* Green */
        font-weight: bold;
    }
     .custom-status-inactive {
        color: #dc3545; /* Red */
        font-weight: bold;
    }


    /* Copyright Footer */
    .footer {
        text-align: center;
        color: gray;
        margin-top: 3rem;
        padding: 1rem;
        font-size: 0.9em;
        border-top: 1px solid #e0e0e0;
    }
    .security-info {
        background-color: #f8f9fa;
        padding: 15px;
        border-radius: 5px;
        margin-top: 10px;
        border: 1px solid #e9ecef;
        display: flex;
        align-items: center;
        justify-content: center; /* Center content */
        margin-bottom: 20px; /* Add space below */
    }
     .security-info span:first-child { /* Icon */
        color: #1f77b4;
        font-size: 1.2em;
        margin-right: 8px;
    }
     .security-info span:last-child { /* Text */
        color: #666;
        font-size: 0.9em;
    }

</style>
""", unsafe_allow_html=True)


# ===============================================
# Fonctions de Visualisation (Peu de changements n√©cessaires ici)
# ===============================================

def create_monthly_trend_chart(df: pd.DataFrame):
    """Cr√©e un graphique lin√©aire de la tendance mensuelle des transactions."""
    if df.empty or 'month' not in df.columns:
        return go.Figure().update_layout(title='Aucune donn√©e pour la tendance mensuelle', xaxis={'visible': False}, yaxis={'visible': False})
    monthly_data = df.groupby('month').size().reset_index(name='transactions')
    fig = px.line(monthly_data, x='month', y='transactions',
                  title='Tendance Mensuelle (Transactions)',
                  labels={'transactions': 'Nb Transactions', 'month': 'Mois'},
                  markers=True) # Ajoute des marqueurs pour la visibilit√©
    fig.update_layout(height=350, title_x=0.5) # Centrer le titre
    return fig

def create_direction_pie_chart(df: pd.DataFrame):
    """Cr√©e un diagramme circulaire de la r√©partition par direction."""
    if df.empty or 'direction' not in df.columns:
        return go.Figure().update_layout(title='Aucune donn√©e pour la r√©partition par direction', xaxis={'visible': False}, yaxis={'visible': False})
    direction_data = df.groupby('direction').size().reset_index(name='transactions')
    fig = px.pie(direction_data, names='direction', values='transactions',
                 title='R√©partition par Direction (Transactions)',
                 hole=0.3) # Donne un effet Donut
    fig.update_layout(showlegend=True, height=350, title_x=0.5)
    return fig

def create_weekday_bar_chart(df: pd.DataFrame):
    """Cr√©e un graphique en barres des transactions par jour de la semaine."""
    if df.empty or 'day_of_week' not in df.columns:
        return go.Figure().update_layout(title='Aucune donn√©e par jour de semaine', xaxis={'visible': False}, yaxis={'visible': False})
    weekday_data = df.groupby('day_of_week').size().reset_index(name='transactions')
    ordre_jours = ['Lundi','Mardi','Mercredi','Jeudi','Vendredi','Samedi','Dimanche']
    weekday_data['day_of_week'] = pd.Categorical(weekday_data['day_of_week'],
                                                 categories=ordre_jours,
                                                 ordered=True)
    weekday_data = weekday_data.sort_values('day_of_week')

    fig = px.bar(weekday_data, x='day_of_week', y='transactions',
                 title='Transactions par Jour de la Semaine',
                 labels={'transactions': 'Nb Transactions', 'day_of_week': 'Jour'},
                 color='day_of_week', text_auto=True) # Affiche les valeurs sur les barres
    fig.update_layout(showlegend=False, height=350, title_x=0.5)
    fig.update_traces(textposition='outside')
    # fig.update_layout(uniformtext_minsize=8, uniformtext_mode='hide') # Peut cacher des textes si barres petites
    return fig

def create_hourly_heatmap(df: pd.DataFrame):
    """Cr√©e une heatmap de la distribution horaire par jour."""
    if df.empty or 'day_of_week' not in df.columns or 'hour' not in df.columns:
         return go.Figure().update_layout(title='Aucune donn√©e pour la heatmap horaire', xaxis={'visible': False}, yaxis={'visible': False})
    hourly_data = df.groupby(['day_of_week', 'hour']).size().reset_index(name='transactions')
    pivot_data = pd.pivot_table(hourly_data, index='day_of_week', columns='hour', values='transactions', fill_value=0)

    ordre_jours = ['Lundi','Mardi','Mercredi','Jeudi','Vendredi','Samedi','Dimanche']
    pivot_data = pivot_data.reindex(ordre_jours, fill_value=0) # Assure l'ordre et inclut les jours sans transaction
    # S'assurer que toutes les heures 0-23 sont pr√©sentes
    all_hours = list(range(24))
    pivot_data = pivot_data.reindex(columns=all_hours, fill_value=0)


    fig = go.Figure(data=go.Heatmap(
        z=pivot_data.values,
        x=[f"{h}h" for h in pivot_data.columns],
        y=pivot_data.index,
        colorscale='Viridis',
        colorbar={'title': 'Nb Transactions'}
    ))
    fig.update_layout(
        title='Heatmap Horaire des Transactions (par Jour)',
        xaxis_title='Heure de la journ√©e',
        yaxis_title='Jour de la semaine',
        height=400,
        title_x=0.5
    )
    return fig

def create_daily_trend_chart(df: pd.DataFrame):
    """Cr√©e un graphique lin√©aire de la tendance journali√®re des transactions."""
    if df.empty or 'date' not in df.columns:
        return go.Figure().update_layout(title='Aucune donn√©e pour la tendance journali√®re', xaxis={'visible': False}, yaxis={'visible': False})
    # S'assurer que 'date' est bien de type date pour le groupement
    df['date_col'] = pd.to_datetime(df['date'])
    daily_data = df.groupby(df['date_col'].dt.date).size().reset_index(name='transactions')
    fig = px.line(daily_data, x='date_col', y='transactions',
                  title='Tendance Journali√®re des Transactions',
                  labels={'transactions': 'Nb Transactions', 'date_col': 'Date'})
    fig.update_layout(showlegend=False, height=350, title_x=0.5)
    return fig

def create_recharge_trend_chart(series: pd.Series):
    """Cr√©e un graphique lin√©aire de la tendance mensuelle des recharges."""
    if series.empty:
        return go.Figure().update_layout(title='Aucune donn√©e pour la tendance des recharges', xaxis={'visible': False}, yaxis={'visible': False})

    # Assurez-vous que l'index est tri√© (il devrait l'√™tre si 'month' est YYYY-MM)
    series = series.sort_index()

    fig = px.line(x=series.index, y=series.values,
                  title='Montant Total des Recharges par Mois',
                  labels={'x': 'Mois', 'y': 'Montant Recharge (XOF)'},
                  markers=True)
    fig.update_layout(height=350, title_x=0.5)
    return fig

def create_peak_hours_chart(df_pivot: pd.DataFrame):
    """Cr√©e un graphique (heatmap ou barres empil√©es) des heures de pointe par direction."""
    if df_pivot.empty:
        return go.Figure().update_layout(title='Aucune donn√©e pour les heures de pointe', xaxis={'visible': False}, yaxis={'visible': False})

    # Heatmap est souvent plus lisible pour ce type de donn√©es
    fig = go.Figure(data=go.Heatmap(
        z=df_pivot.values,
        x=[f"{h}h" for h in df_pivot.columns],
        y=df_pivot.index,
        colorscale='Reds', # Choisir une √©chelle de couleurs appropri√©e
        colorbar={'title': 'Nb Transactions'}
    ))
    fig.update_layout(
        title='Heures de Pointe par Direction (Heatmap)',
        xaxis_title='Heure de la journ√©e',
        yaxis_title='Direction',
        height=400 + len(df_pivot.index) * 20, # Ajuster la hauteur selon le nombre de directions
        title_x=0.5
    )
    return fig


# ===============================================
# Fonctions d'affichage de l'UI Streamlit
# ===============================================

@st.cache_resource # Cache l'instance de l'analyzer
def get_analyzer():
    """Retourne une instance de AutorouteAnalyzer."""
    logging.info("Cr√©ation d'une nouvelle instance de AutorouteAnalyzer.")
    return AutorouteAnalyzer()

@st.cache_data # Cache les donn√©es charg√©es et pars√©es
def load_and_parse_data(_analyzer: AutorouteAnalyzer, uploaded_pdfs: list, uploaded_excel: BytesIO) -> bool:
    """Charge le fichier Excel et parse les PDFs. Retourne True si succ√®s."""
    if not uploaded_pdfs or not uploaded_excel:
        st.warning("Veuillez charger les fichiers PDF et le fichier Excel.")
        return False

    with tempfile.TemporaryDirectory() as temp_dir:
        try:
            # Sauvegarde temporaire du fichier Excel
            excel_path = Path(temp_dir) / "Liste_cartes_Rapido_temp.xlsx" # Nom unique temporaire
            with open(excel_path, "wb") as f:
                f.write(uploaded_excel.getvalue()) # Utiliser getvalue() pour BytesIO

            # Charger les propri√©taires de cartes
            _analyzer.load_card_owners(str(excel_path))
            if not _analyzer.card_owners:
                 st.error("Impossible de charger les propri√©taires de cartes. V√©rifiez le fichier Excel.")
                 return False

            # Parser les fichiers PDF
            # Utiliser st.spinner pour montrer l'activit√©
            with st.spinner(f"Traitement de {len(uploaded_pdfs)} fichier(s) PDF..."):
                _analyzer.all_transactions = [] # R√©initialiser avant de parser
                for pdf_file in uploaded_pdfs:
                    try:
                        # Extraire le num√©ro de compte du nom de fichier (adapter si n√©cessaire)
                        account_number = Path(pdf_file.name).stem
                        pdf_content = BytesIO(pdf_file.getvalue())
                        _analyzer.parse_pdf_content(pdf_content, account_number)
                    except Exception as e:
                        st.error(f"Erreur lors du traitement du fichier PDF '{pdf_file.name}': {e}")
                        logging.error(f"Erreur traitement PDF {pdf_file.name}", exc_info=True)
                        # D√©cider si on continue ou arr√™te en cas d'erreur sur un fichier
                        # return False # Arr√™ter si une erreur est critique

                _analyzer.finalize_data_loading() # Cr√©e et enrichit le DataFrame final

            if _analyzer.raw_df.empty:
                 st.warning("Aucune transaction n'a pu √™tre extraite des fichiers PDF fournis.")
                 return False

            logging.info("Chargement et parsing des donn√©es termin√©s.")
            return True # Succ√®s

        except Exception as e:
            st.error(f"Une erreur g√©n√©rale est survenue lors du chargement des donn√©es: {e}")
            logging.error("Erreur g√©n√©rale chargement donn√©es", exc_info=True)
            return False

@st.cache_data # Cache les r√©sultats des analyses
def run_analyses(_analyzer: AutorouteAnalyzer, inactivity_threshold: int):
    """Ex√©cute toutes les analyses sur les donn√©es charg√©es."""
    if not _analyzer.is_data_loaded:
         st.warning("Les donn√©es ne sont pas charg√©es, impossible d'ex√©cuter les analyses.")
         return {}
    logging.info(f"Ex√©cution des analyses avec seuil d'inactivit√©: {inactivity_threshold} jours.")
    return _analyzer.run_all_analyses(inactivity_threshold)


def display_sidebar(analyzer):
    """Affiche les √©l√©ments de la barre lat√©rale et retourne les param√®tres."""
    with st.sidebar:
        st.image("https://www.dalmotors.com/web/image/website/1/logo/DAL%20Motors?unique=ce05409", width=150) # Exemple d'ajout de logo

        st.markdown("---")
        # Affichage s√©curit√© et copyright
        st.markdown(
            """
            <div class="security-info">
                <span style="color: #1f77b4; font-size: 1.2em; margin-right: 8px;">üîí</span>
                <span style="color: #666; font-size: 0.9em;">
                Application locale - Traitement en m√©moire.
                </span>
            </div>
            """, unsafe_allow_html=True)

        st.header("Param√®tres")
        # Heures non ouvr√©es
        st.subheader("Heures Non Ouvr√©es")
        start_non_work = st.slider("Heure de d√©but (non ouvr√©e)", min_value=0, max_value=23, value=analyzer.start_non_work_hour, key="start_non_work")
        end_non_work = st.slider("Heure de fin (non ouvr√©e)", min_value=0, max_value=23, value=analyzer.end_non_work_hour, key="end_non_work")
        # Mettre √† jour l'analyzer si les heures changent
        if start_non_work != analyzer.start_non_work_hour or end_non_work != analyzer.end_non_work_hour:
             analyzer.set_non_working_hours(start_non_work, end_non_work)
             # Note: Streamlit va r√©-ex√©cuter le script. Si les donn√©es sont cach√©es,
             # il faudra peut-√™tre invalider le cache ou recalculer l'analyse si les heures changent.
             # Pour l'instant, on met juste √† jour l'analyzer. Le recalcul se fera au prochain run_analyses.

        st.markdown("---")
        # Seuil d'inactivit√©
        st.subheader("Activit√© des Cartes")
        inactivity_threshold = st.slider(
            "Seuil d'inactivit√© (jours)",
            min_value=30, max_value=365, value=90, step=15, # Steps plus grands
            help="Nombre de jours sans transaction apr√®s lequel une carte est consid√©r√©e inactive.",
            key="inactivity_slider"
        )

        st.markdown("---")
        st.header("Chargement des Donn√©es")
        # Fichiers PDF
        uploaded_pdfs = st.file_uploader(
             "1. Fichiers PDF des relev√©s Rapido",
             type=['pdf'],
             accept_multiple_files=True,
             help="Chargez un ou plusieurs fichiers PDF contenant les transactions."
        )
        # Fichier Excel
        uploaded_excel = st.file_uploader(
            "2. Fichier Excel des cartes ('Liste cartes Rapido.xlsx')",
            type=['xlsx', 'xls'],
             accept_multiple_files=False,
             help="Chargez le fichier Excel associant les num√©ros de carte aux directions/utilisateurs (colonne 'N¬∞CARTERAPIDO' et 'USER')."
        )

        st.markdown("---")
        # Copyright Footer in Sidebar
        st.markdown(
             """
             <div style="text-align:center; color:gray; font-size:0.8em; margin-top: 2rem;">
                 Copyright (c) 2025 DAL/GPR - Tous droits r√©serv√©s
             </div>
             """,
            unsafe_allow_html=True
         )

    return start_non_work, end_non_work, inactivity_threshold, uploaded_pdfs, uploaded_excel


def display_filters(df: pd.DataFrame):
    """Affiche les filtres principaux et retourne les valeurs s√©lectionn√©es."""
    if df.empty:
        st.info("Chargez des donn√©es pour activer les filtres.")
        return None, None, [], [], None # Retourne des valeurs par d√©faut

    st.subheader("Filtres G√©n√©raux")
    col_f1, col_f2, col_f3 = st.columns([2, 2, 1])

    with col_f1:
        # Filtre Date
        min_date = df['datetime'].min().date()
        max_date = df['datetime'].max().date()
        selected_start_date, selected_end_date = st.date_input(
            "P√©riode d'analyse",
            value=(min_date, max_date), # S√©lectionne toute la plage par d√©faut
            min_value=min_date, max_value=max_date,
            key="date_filter"
        )

    with col_f2:
        # Filtre Direction
        all_directions = sorted(df['direction'].unique().tolist())
        directions_with_all = ["Tous"] + all_directions
        selected_directions = st.multiselect(
            "Directions",
            options=directions_with_all,
            default=["Tous"],
            key="direction_filter"
        )
        if "Tous" in selected_directions:
            selected_directions = all_directions # Utiliser toutes les directions si "Tous" est s√©lectionn√©

    with col_f3:
         # Filtres bool√©ens (Weekend, Heures non ouvr√©es)
         st.markdown("**Conditions Sp√©cifiques**") # Utiliser markdown pour le titre
         filter_we = st.checkbox("Weekend uniquement", key="we_filter")
         filter_hh = st.checkbox("Heures non ouvr√©es uniquement", key="hh_filter")
         # Convertir les bool√©ens en 'Oui'/'Non'/None pour la fonction de filtrage
         weekend_status = 'Oui' if filter_we else None
         non_working_status = 'Oui' if filter_hh else None


    # Barre de recherche globale
    search_term = st.text_input(
        "Rechercher (N¬∞ carte, Montant...)",
        placeholder="Entrez un num√©ro de carte, montant...",
        key="search_bar"
    )

    return selected_start_date, selected_end_date, selected_directions, [weekend_status, non_working_status], search_term

def display_main_table(df: pd.DataFrame):
    """Affiche le tableau principal des transactions filtr√©es."""
    st.subheader("D√©tail des Transactions Filtr√©es")
    if df.empty:
        st.info("Aucune transaction ne correspond aux filtres s√©lectionn√©s.")
        return

    st.write(f"Nombre de transactions affich√©es : **{len(df)}**")

    # Pr√©parer le DataFrame pour l'affichage (s√©lection/renommage des colonnes)
    df_display = df.rename(columns={
        'account': 'N¬∞ Carte', 'datetime': 'Date & Heure', 'type': 'Type',
        'amount': 'Montant (XOF)', 'direction': 'Direction', 'date': 'Date', 'hour': 'H',
        'day_of_week': 'Jour', 'is_weekend': 'WE', 'is_non_working_hours': 'H. Non Ouvr.'
    })

    # S√©lectionner et ordonner les colonnes pour l'affichage
    cols_to_show = ['N¬∞ Carte', 'Direction', 'Date & Heure', 'Type', 'Montant (XOF)', 'Jour', 'H', 'WE', 'H. Non Ouvr.']
    df_display_final = df_display[[col for col in cols_to_show if col in df_display.columns]]

    # Afficher avec st.dataframe pour l'interactivit√©
    st.dataframe(df_display_final, use_container_width=True, height=400) # Hauteur ajustable

    # Bouton de t√©l√©chargement pour le tableau filtr√©
    excel_bytes = to_excel(df_display_final)
    if excel_bytes:
        st.download_button(
            label="üì• T√©l√©charger Tableau Filtr√© (Excel)",
            data=excel_bytes,
            file_name='transactions_filtrees.xlsx',
            mime='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',
            key='download_filtered_table'
        )

def display_recharge_info(filtered_df: pd.DataFrame, analyses_results: dict):
    """Affiche les informations sur les recharges."""
    st.subheader("Analyse des Recharges")

    # 1. Tableau r√©capitulatif des recharges par direction (bas√© sur l'analyse pr√©-calcul√©e et filtr√©e)
    recharge_by_direction = analyses_results.get('recharges_par_direction', pd.DataFrame())

    # Filtrer ce tableau r√©capitulatif en fonction des directions s√©lectionn√©es dans l'UI principale
    # (Si on veut que ce tableau refl√®te aussi le filtre direction de l'UI)
    # directions_filter = st.session_state.get('direction_filter', ["Tous"]) # R√©cup√©rer le filtre actuel
    # if "Tous" not in directions_filter and not recharge_by_direction.empty:
    #     recharge_by_direction = recharge_by_direction[recharge_by_direction['Direction'].isin(directions_filter)]

    if not recharge_by_direction.empty:
        st.write("Montant total des recharges par direction (p√©riode s√©lectionn√©e) :")
        st.dataframe(recharge_by_direction, use_container_width=True)
        excel_bytes_recharge_dir = to_excel(recharge_by_direction)
        if excel_bytes_recharge_dir:
            st.download_button(
                label="üì• T√©l√©charger Recharges par Direction (Excel)",
                data=excel_bytes_recharge_dir,
                file_name='recharges_par_direction.xlsx',
                mime='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',
                key='download_recharge_dir'
            )
    else:
        st.info("Aucune transaction de recharge trouv√©e pour la p√©riode/directions s√©lectionn√©es.")

    # 2. Tableau d√©taill√© des transactions de recharge (bas√© sur le DF d√©j√† filtr√© par l'UI)
    st.write("D√©tail des transactions de recharge (p√©riode/filtres s√©lectionn√©s) :")
    df_recharge_detail = filtered_df[filtered_df['type'].str.lower() == 'recharge'].copy()

    if not df_recharge_detail.empty:
         # Pr√©parer pour l'affichage
         df_recharge_display = df_recharge_detail.rename(columns={
             'account': 'N¬∞ Carte', 'datetime': 'Date & Heure', 'type': 'Type',
             'amount': 'Montant (XOF)', 'direction': 'Direction', 'date': 'Date', 'hour': 'H',
             'day_of_week': 'Jour', 'is_weekend': 'WE', 'is_non_working_hours': 'H. Non Ouvr.'
         })
         cols_recharge = ['N¬∞ Carte', 'Direction', 'Date & Heure', 'Montant (XOF)']
         st.dataframe(df_recharge_display[cols_recharge], use_container_width=True)

         excel_bytes_recharge_detail = to_excel(df_recharge_display[cols_recharge])
         if excel_bytes_recharge_detail:
            st.download_button(
                 label="üì• T√©l√©charger D√©tail Recharges (Excel)",
                 data=excel_bytes_recharge_detail,
                 file_name='details_recharges.xlsx',
                 mime='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',
                 key='download_recharge_detail'
            )
    else:
         st.info("Aucune transaction de recharge d√©taill√©e √† afficher avec les filtres actuels.")


def display_kpis(df: pd.DataFrame, analyses: dict, inactivity_threshold: int):
    """Affiche les Indicateurs Cl√©s de Performance (KPIs)."""
    st.subheader("Indicateurs Cl√©s (P√©riode S√©lectionn√©e)")

    if df.empty:
        st.warning("Aucune donn√©e √† analyser pour la p√©riode s√©lectionn√©e.")
        return

    total_transactions = len(df)
    total_amount_transit = df[df['type'].str.lower() == 'transit']['amount'].sum()
    total_recharge_amount = analyses.get('recharges_mensuelles', pd.Series(dtype=float)).sum() # Approximation sur toute la p√©riode
    total_promo_amount = df[df['type'].str.lower() == 'promotion']['amount'].sum()
    total_chargements = total_recharge_amount + total_promo_amount

    active_cards_period = df['account'].nunique()

    # Recalculer l'activit√© sp√©cifiquement pour la p√©riode filtr√©e pour les KPIs
    # Utiliser une fonction helper ou recalculer ici rapidement
    # Note: `_analyze_card_activity` attend le df et le seuil.
    # Attention: Ne pas appeler la m√©thode priv√©e directement si possible.
    # Solution propre : avoir une m√©thode publique dans l'analyzer qui prend un df et retourne l'activit√©.
    # Ici, pour simplifier, on acc√®de aux r√©sultats pr√©-calcul√©s et on esp√®re qu'ils correspondent √† la p√©riode,
    # ou on affiche les stats globales si on ne recalcule pas.
    # -> Pour l'instant, utilisons les stats globales d'activit√© issues de `run_analyses`
    activite_summary = analyses.get('activite_cartes_resume', pd.DataFrame())
    total_cards_all = 0
    active_cards_all = 0
    inactive_cards_all = 0
    pct_active_all = 0
    pct_inactive_all = 0

    if not activite_summary.empty and 'Total' in activite_summary['Direction'].values:
        total_row = activite_summary[activite_summary['Direction'] == 'Total'].iloc[0]
        total_cards_all = total_row['Total_cartes']
        active_cards_all = total_row['Cartes_actives']
        inactive_cards_all = total_row['Cartes_inactives']
        if total_cards_all > 0:
            pct_active_all = (active_cards_all / total_cards_all * 100)
            pct_inactive_all = (inactive_cards_all / total_cards_all * 100)


    # Passages WE/HH sur la p√©riode filtr√©e
    non_working_hours_transactions = df[df['is_non_working_hours'] & ~df['is_weekend']]
    weekend_transactions = df[df['is_weekend']]
    montant_hh = non_working_hours_transactions[non_working_hours_transactions['type'].str.lower() == 'transit']['amount'].sum()
    montant_we = weekend_transactions[weekend_transactions['type'].str.lower() == 'transit']['amount'].sum()
    pct_non_work = (len(non_working_hours_transactions) / total_transactions * 100) if total_transactions else 0
    pct_we = (len(weekend_transactions) / total_transactions * 100) if total_transactions else 0

    # Affichage des KPIs
    col_k1, col_k2, col_k3 = st.columns(3)
    with col_k1:
        st.metric("Transactions (p√©riode)", f"{total_transactions:,}")
        st.metric("Montant Transit (p√©riode)", f"{total_amount_transit:,.0f} XOF")
        # st.metric("Montant Chargements (Total)", f"{total_chargements:,.0f} XOF") # Peut √™tre trompeur si bas√© sur analyse globale
    with col_k2:
        st.metric("Cartes Actives (p√©riode)", f"{active_cards_period}")
        st.metric("Passages WE (p√©riode)", f"{pct_we:.1f}%", f"{montant_we:,.0f} XOF")
        st.metric("Passages H. Non Ouvr. (p√©riode, L-V)", f"{pct_non_work:.1f}%", f"{montant_hh:,.0f} XOF")
    with col_k3:
        st.metric(f"Total Cartes Connues", f"{total_cards_all}")
        st.metric(f"Actives ({inactivity_threshold}j)", f"{active_cards_all}", f"{pct_active_all:.1f}%")
        st.metric(f"Inactives ({inactivity_threshold}j)", f"{inactive_cards_all}", f"{pct_inactive_all:.1f}%")


def display_visualizations(df: pd.DataFrame, analyses_results: dict):
    """Affiche les graphiques principaux."""
    st.subheader("Visualisations (P√©riode S√©lectionn√©e)")

    if df.empty:
        st.warning("Aucune donn√©e √† visualiser pour la p√©riode s√©lectionn√©e.")
        return

    # Cr√©er une copie pour √©viter les SettingWithCopyWarning si on modifie df_visu
    df_visu = df.copy()

    # Filtrage suppl√©mentaire pour les visualisations (Mois/Jour sp√©cifique)
    # Note: Ce filtrage s'applique *apr√®s* les filtres g√©n√©raux
    col_v1, col_v2 = st.columns(2)
    with col_v1:
         available_months = ["Tous"] + sorted(df_visu['month'].unique().tolist())
         selected_month = st.selectbox(
             "Affiner par mois",
             options=available_months,
             index=0, # Default to "Tous"
             key="visu_month_filter"
         )
         if selected_month != "Tous":
             df_visu = df_visu[df_visu['month'] == selected_month]

    with col_v2:
        available_days = ["Tous"]
        if not df_visu.empty:
             # Convertir en date avant de trier
             unique_dates = pd.to_datetime(df_visu['date']).dt.date.unique()
             available_days.extend(sorted(unique_dates))

        selected_day = st.selectbox(
             "Affiner par jour",
             options=available_days,
             index=0, # Default to "Tous"
             key="visu_day_filter"
         )
        if selected_day != "Tous":
             df_visu = df_visu[pd.to_datetime(df_visu['date']).dt.date == selected_day]


    # Afficher les graphiques bas√©s sur df_visu (doublement filtr√©)
    if df_visu.empty:
        st.info("Aucune donn√©e pour les filtres de visualisation s√©lectionn√©s.")
        return

    st.markdown("---") # S√©parateur visuel

    # Disposition des graphiques
    col1, col2 = st.columns(2)
    with col1:
        st.plotly_chart(create_monthly_trend_chart(df_visu), use_container_width=True)
        st.plotly_chart(create_weekday_bar_chart(df_visu), use_container_width=True)
        # Afficher la tendance journali√®re si un mois est s√©lectionn√© mais pas un jour pr√©cis
        if selected_month != "Tous" and selected_day == "Tous":
            st.plotly_chart(create_daily_trend_chart(df_visu), use_container_width=True)

    with col2:
        st.plotly_chart(create_direction_pie_chart(df_visu), use_container_width=True)
        st.plotly_chart(create_hourly_heatmap(df_visu), use_container_width=True)
        # Afficher la tendance des recharges (bas√©e sur les donn√©es globales filtr√©es par mois/jour)
        recharges_trend_data = df_visu[df_visu['type'].str.lower() == 'recharge'].groupby('month')['amount'].sum()
        st.plotly_chart(create_recharge_trend_chart(recharges_trend_data), use_container_width=True)


    # Afficher l'analyse des heures de pointe (bas√©e sur df_visu)
    st.markdown("---")
    st.subheader("Analyse des Heures de Pointe (p√©riode/filtres affin√©s)")
    peak_hours_data = df_visu.groupby(['direction', 'hour']).size().unstack(fill_value=0)
    # S'assurer que toutes les heures sont pr√©sentes
    all_hours = list(range(24))
    peak_hours_data = peak_hours_data.reindex(columns=all_hours, fill_value=0)

    if not peak_hours_data.empty:
        # Choix entre Table et Graphique
        view_type = st.radio("Vue Heures de Pointe:", ["Tableau", "Graphique (Heatmap)"], index=1, key="peak_hours_view", horizontal=True)
        if view_type == "Tableau":
            st.dataframe(peak_hours_data, use_container_width=True)
        else:
            st.plotly_chart(create_peak_hours_chart(peak_hours_data), use_container_width=True)

        excel_bytes_peak = to_excel(peak_hours_data.reset_index()) # Reset index pour export
        if excel_bytes_peak:
             st.download_button(
                 label="üì• T√©l√©charger Heures de Pointe (Excel)",
                 data=excel_bytes_peak,
                 file_name='heures_pointe_par_direction.xlsx',
                 mime='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',
                 key='download_peak_hours'
             )
    else:
        st.info("Aucune donn√©e de transaction pour analyser les heures de pointe avec les filtres actuels.")


def display_card_analysis(df: pd.DataFrame, analyzer: AutorouteAnalyzer, inactivity_threshold: int):
    """Affiche l'analyse sp√©cifique √† une carte recherch√©e."""
    st.header("üîç Analyse par Carte Individuelle")
    card_search_term = st.text_input(
        "Rechercher une carte par son num√©ro",
        key="card_search_input",
        placeholder="Entrez le num√©ro exact de la carte"
    )

    if card_search_term:
        card_search_term = card_search_term.strip() # Nettoyer l'input
        # Filtrer le DataFrame global (non filtr√© par date/direction ici) pour cette carte
        card_df = df[df['account'] == card_search_term]

        if not card_df.empty:
            st.subheader(f"Analyse pour la carte : {card_search_term}")

            # 1. Informations g√©n√©rales et statut d'activit√©
            activite_details = analyzer.analysis_results.get('activite_cartes_details', pd.DataFrame())
            card_activity_info = activite_details[activite_details['Num√©ro_carte_Rapido'] == card_search_term]

            if not card_activity_info.empty:
                info = card_activity_info.iloc[0]
                status_class = "custom-status-active" if info['Statut'] == "Active" else "custom-status-inactive"
                st.markdown(f"""
                    <div class='custom-info-box'>
                        Direction: **{info['Direction']}**<br>
                        Statut (seuil {inactivity_threshold}j): <span class='{status_class}'>{info['Statut']}</span><br>
                        Derni√®re transaction: {info['Derni√®re_transaction'] if pd.notna(info['Derni√®re_transaction']) else 'N/A'}<br>
                        Jours depuis derni√®re transaction: {info['Jours_depuis_derniere_transaction'] if pd.notna(info['Jours_depuis_derniere_transaction']) else 'N/A'}
                    </div>
                """, unsafe_allow_html=True)
            else:
                st.warning(f"Statut d'activit√© non trouv√© pour la carte {card_search_term} (peut-√™tre inconnue?).")

            # 2. Statistiques cl√©s pour cette carte (sur la p√©riode globale des donn√©es charg√©es)
            total_trans = len(card_df)
            total_transit_amount = card_df[card_df['type'].str.lower() == 'transit']['amount'].sum()
            total_recharge_amount = card_df[card_df['type'].str.lower() == 'recharge']['amount'].sum()

            st.metric("Nb Total Transactions (carte)", f"{total_trans}")
            st.metric("Montant Total Transit (carte)", f"{total_transit_amount:,.0f} XOF")
            st.metric("Montant Total Recharges (carte)", f"{total_recharge_amount:,.0f} XOF")


            # 3. Tableau des transactions de la carte (WE ou HH)
            st.subheader("Transactions Weekend ou Hors Heures Ouvr√©es (Carte)")
            card_df_we_hh = card_df[card_df['is_weekend'] | card_df['is_non_working_hours']]

            if not card_df_we_hh.empty:
                # Pr√©parer pour affichage
                card_df_display = card_df_we_hh.rename(columns={
                     'account': 'N¬∞ Carte', 'datetime': 'Date & Heure', 'type': 'Type',
                     'amount': 'Montant (XOF)', 'direction': 'Direction', 'date': 'Date', 'hour': 'H',
                     'day_of_week': 'Jour', 'is_weekend': 'WE', 'is_non_working_hours': 'H. Non Ouvr.'
                 })
                cols_card = ['Date & Heure', 'Type', 'Montant (XOF)', 'Jour', 'H', 'WE', 'H. Non Ouvr.']
                st.dataframe(card_df_display[cols_card], use_container_width=True)

                excel_bytes_card = to_excel(card_df_display[cols_card])
                if excel_bytes_card:
                     st.download_button(
                         label=f"üì• T√©l√©charger Transactions WE/HH Carte {card_search_term} (Excel)",
                         data=excel_bytes_card,
                         file_name=f'transactions_we_hh_carte_{card_search_term}.xlsx',
                         mime='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',
                         key=f'download_card_{card_search_term}'
                     )
            else:
                 st.info("Cette carte n'a pas de transactions enregistr√©es pendant le week-end ou en dehors des heures ouvr√©es sur la p√©riode charg√©e.")

        else:
            st.warning(f"Aucune transaction trouv√©e pour le num√©ro de carte '{card_search_term}'. V√©rifiez le num√©ro ou si la carte existe dans les relev√©s.")

def display_key_analyses(analyses: dict):
    """Affiche les tableaux d'analyses cl√©s."""
    st.header("üìã Analyses Cl√©s (Bas√©es sur Toutes les Donn√©es Charg√©es)")
    st.caption("Ces tableaux pr√©sentent les analyses sur l'ensemble des donn√©es charg√©es, ind√©pendamment des filtres de date/direction ci-dessus.")

    if not analyses:
        st.warning("Les analyses n'ont pas pu √™tre ex√©cut√©es. Chargez des donn√©es valides.")
        return

    # Utiliser des expanders pour organiser les analyses
    with st.expander("1. Passages Mensuels par Direction", expanded=False):
        df_pass_mens = analyses.get('passages_mensuels_par_direction', pd.DataFrame())
        if not df_pass_mens.empty:
            st.dataframe(df_pass_mens, use_container_width=True)
            excel_bytes = to_excel(df_pass_mens)
            if excel_bytes:
                st.download_button("üì• T√©l√©charger (Excel)", excel_bytes, "passages_mensuels_direction.xlsx", key="dl_pass_mens")
        else:
            st.info("Donn√©es non disponibles.")

    with st.expander("2. Nombre de Passages & Montants par Carte/Direction", expanded=False):
        df_nb_pass = analyses.get('nb_passages_par_carte_direction', pd.DataFrame())
        if not df_nb_pass.empty:
            # Ajout d'une recherche simple pour ce tableau
            search_passages = st.text_input("Rechercher (N¬∞ carte / Direction)", key="search_nb_pass")
            df_to_show = df_nb_pass
            if search_passages:
                df_to_show = df_nb_pass[
                    df_nb_pass['Num√©ro carte Rapido'].astype(str).str.contains(search_passages, case=False, na=False) |
                    df_nb_pass['Direction'].astype(str).str.contains(search_passages, case=False, na=False)
                ]
            st.dataframe(df_to_show, use_container_width=True)
            excel_bytes = to_excel(df_nb_pass) # Exporter tout le tableau
            if excel_bytes:
                st.download_button("üì• T√©l√©charger (Excel)", excel_bytes, "passages_montants_carte_direction.xlsx", key="dl_nb_pass")
        else:
            st.info("Donn√©es non disponibles.")

    with st.expander("3. Nombre de Cartes Actives par Direction (sur p√©riode charg√©e)", expanded=False):
        df_nb_cartes = analyses.get('nb_cartes_par_direction', pd.DataFrame())
        if not df_nb_cartes.empty:
            st.dataframe(df_nb_cartes, use_container_width=True)
            excel_bytes = to_excel(df_nb_cartes)
            if excel_bytes:
                st.download_button("üì• T√©l√©charger (Excel)", excel_bytes, "nb_cartes_actives_direction.xlsx", key="dl_nb_cartes")
        else:
            st.info("Donn√©es non disponibles.")

    with st.expander("4. Cartes avec Consommation Week-end / Hors Heures Ouvr√©es (Transit)", expanded=False):
        df_wehh = analyses.get('cartes_weekend_heures_non_ouvrees', pd.DataFrame())
        if not df_wehh.empty:
            search_wehh = st.text_input("Rechercher (N¬∞ carte / Direction)", key="search_wehh")
            df_to_show = df_wehh
            if search_wehh:
                 df_to_show = df_wehh[
                     df_wehh['Num√©ro carte Rapido'].astype(str).str.contains(search_wehh, case=False, na=False) |
                     df_wehh['Direction'].astype(str).str.contains(search_wehh, case=False, na=False)
                 ]
            st.dataframe(df_to_show, use_container_width=True)
            excel_bytes = to_excel(df_wehh)
            if excel_bytes:
                st.download_button("üì• T√©l√©charger (Excel)", excel_bytes, "cartes_conso_we_hh.xlsx", key="dl_wehh")
        else:
            st.info("Donn√©es non disponibles ou aucune consommation de ce type trouv√©e.")

    with st.expander("5. Analyse d'Activit√© des Cartes (Globale)", expanded=True): # Ouvrir par d√©faut
        df_act_sum = analyses.get('activite_cartes_resume', pd.DataFrame())
        df_act_det = analyses.get('activite_cartes_details', pd.DataFrame())

        st.subheader("R√©sum√© par Direction")
        if not df_act_sum.empty:
            st.dataframe(df_act_sum, use_container_width=True)
            excel_bytes = to_excel(df_act_sum)
            if excel_bytes:
                st.download_button("üì• T√©l√©charger R√©sum√© (Excel)", excel_bytes, "resume_activite_cartes.xlsx", key="dl_act_sum")
        else:
            st.info("R√©sum√© non disponible.")

        st.subheader("D√©tails par Carte")
        if not df_act_det.empty:
            search_act = st.text_input("Rechercher (N¬∞ carte / Direction / Statut)", key="search_act_det")
            df_to_show = df_act_det
            if search_act:
                 search_act_lower = search_act.lower()
                 df_to_show = df_act_det[
                     df_act_det['Num√©ro_carte_Rapido'].astype(str).str.contains(search_act, case=False, na=False) |
                     df_act_det['Direction'].astype(str).str.contains(search_act, case=False, na=False) |
                     df_act_det['Statut'].astype(str).str.lower().str.contains(search_act_lower, na=False)
                 ]
            st.dataframe(df_to_show, use_container_width=True)
            excel_bytes = to_excel(df_act_det)
            if excel_bytes:
                st.download_button("üì• T√©l√©charger D√©tails (Excel)", excel_bytes, "details_activite_cartes.xlsx", key="dl_act_det")
        else:
            st.info("D√©tails non disponibles.")


# ===============================================
# Fonctions principales de l'application
# ===============================================

def module_rapido(analyzer):
    """Affiche l'interface du module d'analyse Rapido."""
    st.title("üìä Analyseur de Relev√©s Rapido")

    # --- Sidebar et chargement des donn√©es ---
    start_h, end_h, inactivity_th, pdfs, excel = display_sidebar(analyzer)

    data_loaded = False
    analysis_results = {}
    df_raw = pd.DataFrame()

    # Logique de chargement et d'analyse d√©clench√©e SI les fichiers sont pr√©sents
    if pdfs and excel:
        # Utiliser l'√©tat de session pour √©viter rechargement si fichiers n'ont pas chang√© ?
        # Pour l'instant, on recharge si les objets fichiers sont l√†.
        # Le cache @st.cache_data g√©rera l'ex√©cution r√©elle.
        data_loaded = load_and_parse_data(analyzer, pdfs, excel)

        if data_loaded and analyzer.is_data_loaded:
             df_raw = analyzer.raw_df
             # Ex√©cuter les analyses apr√®s chargement (sera cach√© aussi)
             analysis_results = run_analyses(analyzer, inactivity_th)
        else:
             st.error("Le chargement ou le parsing des donn√©es a √©chou√©. V√©rifiez les fichiers et les logs.")
             # Assurer que df_raw et analysis_results sont vides si √©chec
             df_raw = pd.DataFrame()
             analysis_results = {}
    else:
        st.info("‚ÑπÔ∏è Veuillez charger les fichiers PDF et Excel requis dans la barre lat√©rale pour d√©marrer l'analyse.")

    # Si les donn√©es sont charg√©es, afficher le reste de l'interface
    if data_loaded and not df_raw.empty:

        # --- Filtres principaux ---
        start_d, end_d, dirs, we_hh_filters, search = display_filters(df_raw)
        weekend_filter, hh_filter = we_hh_filters # D√©paqueter les filtres bool√©ens

        # --- Filtrage des donn√©es pour affichage ---
        # Le filtrage se base maintenant sur les s√©lections de l'UI
        filtered_data = analyzer.get_filtered_data(
             start_date=start_d,
             end_date=end_d,
             directions=dirs,
             weekend_filter=weekend_filter, # Utilise 'Oui' ou None
             non_working_hours_filter=hh_filter, # Utilise 'Oui' ou None
             search_term=search
        )

        # --- Bouton d'Export G√©n√©ral ---
        st.markdown("---")
        st.subheader("Exporter l'Analyse Compl√®te")
        # Utiliser une colonne pour mieux placer le bouton
        col_export1, _ = st.columns([1, 3])
        with col_export1:
            export_filename = f"Analyse_Rapido_{start_d.strftime('%Y%m%d')}_au_{end_d.strftime('%Y%m%d')}.xlsx"
            # Utiliser un NamedTemporaryFile pour l'export
            # Le bouton g√©n√®re l'export √† la vol√©e quand on clique
            if st.button("üìä G√©n√©rer l'Export Excel (P√©riode Filtr√©e)"):
                 with st.spinner("Pr√©paration de l'export Excel..."):
                      try:
                           # Utiliser NamedTemporaryFile pour obtenir un chemin de fichier
                           with tempfile.NamedTemporaryFile(delete=False, suffix=".xlsx") as tmpfile:
                                analyzer.export_analyses_to_excel(
                                    tmpfile.name,
                                    start_date=start_d,
                                    end_date=end_d,
                                    inactivity_threshold=inactivity_th
                                )
                                # Lire le contenu du fichier temporaire pour le download button
                                tmpfile.seek(0)
                                excel_export_data = tmpfile.read()

                           # Stocker les donn√©es dans session_state pour le download button
                           st.session_state['excel_export_data'] = excel_export_data
                           st.session_state['excel_export_filename'] = export_filename

                      except Exception as e:
                           st.error(f"Erreur lors de la g√©n√©ration de l'export : {e}")
                           logging.error("Erreur g√©n√©ration export g√©n√©ral", exc_info=True)
                           st.session_state['excel_export_data'] = None # R√©initialiser en cas d'erreur

        # Afficher le bouton de t√©l√©chargement si les donn√©es sont pr√™tes en session_state
        if 'excel_export_data' in st.session_state and st.session_state['excel_export_data']:
              st.download_button(
                   label="üì• T√©l√©charger l'Analyse Compl√®te",
                   data=st.session_state['excel_export_data'],
                   file_name=st.session_state['excel_export_filename'],
                   mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                   key="download_main_export",
                   # Supprimer les donn√©es de session_state apr√®s le t√©l√©chargement (optionnel)
                   # on_click=lambda: st.session_state.pop('excel_export_data', None)
              )


        st.markdown("---")

        # --- Affichage Tableau Principal ---
        display_main_table(filtered_data)

        st.markdown("---")

        # --- Affichage Infos Recharges ---
        # Passe le DF d√©j√† filtr√© par l'UI et les analyses globales
        display_recharge_info(filtered_data, analysis_results)

        st.markdown("---")

        # --- Affichage KPIs ---
        display_kpis(filtered_data, analysis_results, inactivity_th)

        st.markdown("---")

        # --- Affichage Visualisations ---
        display_visualizations(filtered_data, analysis_results)

        st.markdown("---")

        # --- Affichage Analyse par Carte ---
        # Passe le DF brut global (df_raw) car l'analyse par carte n'est pas li√©e aux filtres g√©n√©raux
        display_card_analysis(df_raw, analyzer, inactivity_th)

        st.markdown("---")

        # --- Affichage Analyses Cl√©s ---
        # Passe les r√©sultats d'analyse globaux
        display_key_analyses(analysis_results)

    # Afficher le footer √† la fin de la page principale
    st.markdown("<div class='footer'>Copyright (c) 2025 DAL/GPR - Tous droits r√©serv√©s</div>", unsafe_allow_html=True)


def module_accueil():
    """Affiche la page d'accueil de l'application."""
    col_logo, col_title = st.columns([1, 5])
    with col_logo:
        st.image("https://www.dalmotors.com/web/image/website/1/logo/DAL%20Motors?unique=ce05409", width=100)
    with col_title:
        st.title("Portail d'Applications d'Analyse")

    st.markdown("---")
    st.markdown(
        """
        Bienvenue sur le portail d'analyse de donn√©es de DAL/GPR.
        S√©lectionnez un module ci-dessous pour commencer.

        <div class="security-info" style="justify-content: flex-start;">
            <span style="color: #1f77b4; font-size: 1.2em; margin-right: 8px;">üîí</span>
            <span style="color: #666; font-size: 0.9em;">
            Application locale s√©curis√©e. Traitement des donn√©es en m√©moire uniquement.
            </span>
        </div>
        """,
        unsafe_allow_html=True
    )

    st.markdown("## Modules Disponibles")

    modules = {
        "Analyse des passages Rapido": {
            "description": "Analyser les relev√©s de transactions des cartes p√©age Rapido.",
            "icon": "üöó",
            "key": "rapido"
        },
        "Analyse Carburant": {
            "description": "Analyser les transactions des cartes carburant.",
            "icon": "‚õΩ",
            "key": "carburant"
        }
    }

    # Afficher les modules sous forme de cartes ou de boutons
    for module_name, info in modules.items():
        st.subheader(f"{info['icon']} {module_name}")
        st.write(info['description'])
        if st.button(f"Lancer {module_name}", key=f"btn_{info['key']}", use_container_width=False): # Bouton plus petit
            st.session_state.module = module_name
            st.rerun() # Forcer la r√©ex√©cution pour changer de module
        st.markdown("---")

    # Footer de la page d'accueil
    st.markdown("<div class='footer'>Copyright (c) 2025 DAL/GPR - Tous droits r√©serv√©s</div>", unsafe_allow_html=True)


def main():
    """Fonction principale qui g√®re la navigation entre les modules."""
    # Initialiser l'analyzer une seule fois et le mettre en cache via get_analyzer()
    analyzer = get_analyzer()

    # Logique de navigation bas√©e sur st.session_state
    if 'module' not in st.session_state or st.session_state.module is None:
        module_accueil()
    else:
        # Afficher le bouton de retour dans la barre lat√©rale si on est dans un module
        if st.sidebar.button("‚Üê Retour √† l'Accueil", key="back_home"):
            st.session_state.module = None
            # Nettoyer potentiellement d'autres √©tats de session sp√©cifiques au module si n√©cessaire
            st.rerun()

        # Ex√©cuter le module s√©lectionn√©
        if st.session_state.module == "Analyse des passages Rapido":
            module_rapido(analyzer)
        elif st.session_state.module == "Analyse Carburant":
            module_carburant_main() # Call the imported main function from carburant.py
        else:
            # Si l'√©tat est invalide, retourner √† l'accueil
            st.warning("Module non reconnu, retour √† l'accueil.")
            st.session_state.module = None
            st.rerun()

if __name__ == "__main__":
    main()

